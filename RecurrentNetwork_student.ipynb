{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb2b6f6",
   "metadata": {},
   "source": [
    "# 1  Recurrent Neural Network\n",
    "\n",
    "In this part, you need to build the RNN unit in the diagram using only numpy .\n",
    "Then you need to build the input to test RNN for no less than 4 time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec00ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575815db",
   "metadata": {},
   "source": [
    "## 1.1 Tanh activation: forward\n",
    "\n",
    "Implement the forward pass for the Tanh activation function in the `tanh_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49e008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_forward(x):\n",
    "    out = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75e3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing tanh_forward function:\n",
      "difference:  [[ 2.73999023e-09  1.33874611e-09  2.58224614e-09 -2.81432277e-09]\n",
      " [ 2.92331293e-09  3.47872175e-09 -3.47872175e-09 -2.92331293e-09]\n",
      " [ 2.81432277e-09 -2.58224614e-09 -1.33874611e-09 -2.73999023e-09]]\n"
     ]
    }
   ],
   "source": [
    "# Test the tanh_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out= tanh_forward(x)\n",
    "correct_out = np.array([[-0.46211716, -0.38770051, -0.30786199, -0.22343882],\n",
    "                        [-0.13552465, -0.04542327,  0.04542327,  0.13552465],\n",
    "                        [ 0.22343882,  0.30786199,  0.38770051,  0.46211716]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing tanh_forward function:')\n",
    "print('difference: ', (out - correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadfd16",
   "metadata": {},
   "source": [
    "## 1.2 Softmax activation: forward\n",
    "\n",
    "Implement the forward pass for the softmax activation function in the `softmax_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "967fac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(x):\n",
    "    e_x = np.exp(x - np.max(x)) # 减去最大值，防止溢出\n",
    "    out = e_x / np.sum(e_x, axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2714fbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing softmax_forward function:\n",
      "difference:  [[ 4.91230565e-09  4.91230562e-09  4.91230562e-09  4.91230562e-09]\n",
      " [-1.19795263e-09 -1.19795263e-09 -1.19795263e-09 -1.19795263e-09]\n",
      " [-3.71435299e-09 -3.71435299e-09 -3.71435299e-09 -3.71435299e-09]]\n"
     ]
    }
   ],
   "source": [
    "# Test the softmax_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out= softmax_forward(x)\n",
    "correct_out = np.array([[0.22182884 ,0.22182884 ,0.22182884 ,0.22182884],\n",
    "                        [0.31911211 ,0.31911211 ,0.31911211 ,0.31911211],\n",
    "                        [0.45905905 ,0.45905905 ,0.45905905 ,0.45905905]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing softmax_forward function:')\n",
    "print('difference: ', (out - correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef0379",
   "metadata": {},
   "source": [
    "## 1.3 Building a single RNN cell according to instructions\n",
    "\n",
    "And you need to test your cell according to the given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d015ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_cell_forward\n",
    "\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Waa, Wax, Wya, ba, by = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['ba'], parameters['by']\n",
    "    \n",
    "    # Compute the hidden state with tanh activation and using your new hidden state a_next compute the prediction yt_pred\n",
    "    W_aa_a_prev, W_ax_xt  = np.dot(Waa, a_prev), np.dot(Wax, xt)\n",
    "    a_next = tanh_forward(W_aa_a_prev + W_ax_xt + ba)\n",
    "    yt_pred = softmax_forward(np.dot(Wya, a_next) + by)\n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed61f606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rnn cell:\n",
      "difference:  [[ 3.46237604e-09  4.11825871e-09 -3.30776050e-09  9.71139542e-10\n",
      "   9.37157874e-10 -2.67774309e-09  3.30873294e-09  4.75652538e-09\n",
      "   3.97501351e-09  3.96566117e-09]\n",
      " [-3.46237605e-09 -4.11825862e-09  3.30776051e-09 -9.71139502e-10\n",
      "  -9.37157907e-10  2.67774314e-09 -3.30873295e-09 -4.75652551e-09\n",
      "  -3.97501354e-09 -3.96566113e-09]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your function\n",
    "xt = np.linspace(-1, 1, num=30).reshape(3, 10)\n",
    "a_prev = np.linspace(-1, 1, num=50).reshape(5, 10)\n",
    "Waa = np.linspace(-1, 1, num=25).reshape(5, 5)\n",
    "Wax = np.linspace(-1, 1, num=15).reshape(5, 3)\n",
    "Wya = np.linspace(-1, 1, num=10).reshape(2, 5)\n",
    "ba = np.linspace(-1, 1, num=5).reshape(5, 1)\n",
    "by = np.linspace(-1, 1, num=2).reshape(2, 1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "correct_yt_pred= np.array([[0.01173649, 0.00919828, 0.00893178, 0.01089865, 0.01526617, 0.02177653,\n",
    "                            0.02945747, 0.03722571, 0.04439407, 0.05064493],\n",
    "                            [0.98826351, 0.99080172, 0.99106822, 0.98910135, 0.98473383, 0.97822347,\n",
    "                            0.97054253, 0.96277429, 0.95560593, 0.94935507]])\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing rnn cell:')\n",
    "print('difference: ', (yt_pred - correct_yt_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf9903",
   "metadata": {},
   "source": [
    "## 1.4 Follow the instructions to construct a RNN network\n",
    "\n",
    "And you need to test your network according to the given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60c4620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_forward\n",
    "\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize \"caches\" which contain the list of all cache\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    # initialize \"a\" and \"y\" with zeros\n",
    "    a, y_pred = np.zeros((n_a, m, T_x)), np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "    \n",
    "        # Update next hidden state, compute the prediction, get the cache\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "    \n",
    "        # Save the value of the new \"next\" hidden state in a\n",
    "        a[:, :, t] = a_next\n",
    "    \n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "    \n",
    "        # Append \"cache\" to caches\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b631fce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rnn network:\n",
      "y_difference:  [ 9.11572484e-10 -4.37519909e-09  5.64888136e-10  3.66363306e-09]\n",
      "a_difference:  [-3.13617465e-10 -2.13382212e-09  8.34690983e-10 -2.64970512e-09]\n",
      "cache_difference:  [-1.68067227e-10  2.52100835e-09 -4.78991598e-09 -2.10084040e-09]\n"
     ]
    }
   ],
   "source": [
    "xt = np.linspace(-1, 1, num=120).reshape(3, 10,4)\n",
    "a0 = np.linspace(-1, 1, num=50).reshape(5, 10)\n",
    "Waa = np.linspace(-1, 1, num=25).reshape(5, 5)\n",
    "Wax = np.linspace(-1, 1, num=15).reshape(5, 3)\n",
    "Wya = np.linspace(-1, 1, num=10).reshape(2, 5)\n",
    "ba = np.linspace(-1, 1, num=5).reshape(5, 1)\n",
    "by = np.linspace(-1, 1, num=2).reshape(2, 1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(xt, a0, parameters)\n",
    "\n",
    "correct_y_pred_1r3c = np.array([0.98938177, 0.92781377, 0.98516102, 0.94756773])\n",
    "correct_a_4r1c = np.array([0.2609307,  0.98545286, 0.84171072, 0.99476474])\n",
    "correct_caches_113 = np.array([-0.12605042, -0.1092437,  -0.09243697, -0.07563025])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing rnn network:')\n",
    "\n",
    "print('y_difference: ', (y_pred[1][3] - correct_y_pred_1r3c))\n",
    "print('a_difference: ', (a[4][1] - correct_a_4r1c))\n",
    "print('cache_difference: ', (caches[1][1][3] - correct_caches_113))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3c56a-239a-4257-a4a9-da523384475f",
   "metadata": {},
   "source": [
    "# 2  Backpropagation in recurrent neural networks\n",
    "\n",
    "In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae821c4-3405-4f19-9ea7-0f16ee2e1070",
   "metadata": {},
   "source": [
    "## 2.1 - Basic RNN backward pass\n",
    "\n",
    "We will start by computing the backward pass for the basic RNN-cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4962253-126c-46bb-ab38-e03d82432eb4",
   "metadata": {},
   "source": [
    "### Deriving the one step backward functions:\n",
    "\n",
    "To compute the rnn_cell_backward you need to compute the reference image in the homework. It is a good exercise to derive them by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3d6eff6-85f7-4e56-969c-d3423e573069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradients of input data, of shape (n_x, m)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve values from cache\n",
    "    a_next, a_prev, xt, parameters = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Waa, Wax, _, _, _ = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['ba'], parameters['by']\n",
    " \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
    "    dtanh = (1 - a_next**2) * da_next\n",
    "    \n",
    "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
    "    dxt = np.dot(Wax.T, dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    " \n",
    "    # compute the gradient with respect to Waa (≈2 lines)\n",
    "    da_prev = np.dot(Waa.T, dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    " \n",
    "    # compute the gradient with respect to b (≈1 line)\n",
    "    dba = np.sum(dtanh, axis=1, keepdims=True)  # 按行求和\n",
    " \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ea96-0c9d-46b5-b0de-35c4f2ec79e9",
   "metadata": {},
   "source": [
    "### test the backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4745fed8-4d2c-4616-b218-ed5543197d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.15389161e-09  3.65399597e-09  2.11909662e-09  1.62985550e-09\n",
      " -7.39439565e-10 -9.61219659e-10 -3.89284660e-10 -1.21954563e-09\n",
      " -2.13211884e-09  1.41260277e-09]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "correct_gra_xt_2 = [-0.02886535,  0.15285269, -0.24771553, -0.1538868,  -0.30123978,\n",
    "                    0.43068929,   0.1448896,   0.07010807, -0.22645813,  0.09275214]\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(gradients['dxt'][2]-correct_gra_xt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f26fe8-5aa3-41c2-b200-a9a917a95ba6",
   "metadata": {},
   "source": [
    "## 2.2 Backward pass through the RNN\n",
    "\n",
    "\n",
    "Implement the rnn_backward function. Initialize the return variables with zeros first and then loop through all the time steps while calling the rnn_cell_backward at each time timestep, update the other variables accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "78abd6d3-53d2-44c0-874c-b2f1479e93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "​\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
    "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
    "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)\n",
    "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
    "    # _caches, x = caches\n",
    "    _caches, _ = caches\n",
    "    # a1, a0, x1, parameters = _caches[0]\n",
    "    _, _, x1, _ = _caches[0]\n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    # initialize the gradients with the right sizes (≈6 lines)\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    dWax = np.zeros((n_a, n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, 1))\n",
    "    da_prev = np.zeros((n_a, m))\n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prev, _caches[t])    # 计算每一个时间步的梯度\n",
    "\n",
    "        # Retrieve derivatives from gradients (≈ 1 line)\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        \n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
    "    da0 = da_prevt\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\":dx, \"da0\":da0, \"dWax\":dWax, \"dWaa\":dWaa, \"dba\":dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7923d4-d69b-4f15-8424-2e6c44322e57",
   "metadata": {},
   "source": [
    "### test the function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6320ed9b-fe05-4267-9ffc-cab4b396fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-0.15028183 -0.34554547  0.02071758  0.01483317]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.17268893183890804\n",
      "gradients[\"da0\"].shape = (5, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0e489-007e-40c4-a516-3c0bcd261955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
